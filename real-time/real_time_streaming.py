import os
import psutil
import subprocess
import time
import numpy as np
import pandas as pd
import joblib
import signal
import warnings
warnings.filterwarnings("ignore")

# Directory to store logs generated by Zeek
LOG_DIR = "real-time/zeek_logs"
RAW_CSV = "real-time/raw_captured_data.csv"
FINAL_CSV = "real-time/final_captured_data.csv"

# Mapping of log file columns to the final DataFrame columns
FIELD_MAPPINGS = {
    "conn.log": {
        "ts": "ts", "id.orig_h": "src_ip", "id.orig_p": "src_port",
        "id.resp_h": "dst_ip", "id.resp_p": "dst_port", "proto": "proto",
        "service": "service", "duration": "duration", "orig_bytes": "src_bytes",
        "resp_bytes": "dst_bytes", "conn_state": "conn_state", "missed_bytes": "missed_bytes",
        "orig_pkts": "src_pkts", "orig_ip_bytes": "src_ip_bytes", "resp_pkts": "dst_pkts",
        "resp_ip_bytes": "dst_ip_bytes"
    },
    "dns.log": {
        "ts": "ts", "id.orig_h": "src_ip", "id.orig_p": "src_port",
        "id.resp_h": "dst_ip", "id.resp_p": "dst_port", "proto": "proto",
        "query": "dns_query", "qclass": "dns_qclass", "qtype": "dns_qtype",
        "rcode": "dns_rcode", "AA": "dns_AA", "RD": "dns_RD", "RA": "dns_RA", "rejected": "dns_rejected"
    },
    "ssl.log": {
        "ts": "ts", "id.orig_h": "src_ip", "id.orig_p": "src_port", "id.resp_h": "dst_ip", "id.resp_p": "dst_port",
        "version": "ssl_version", "cipher": "ssl_cipher", "resumed": "ssl_resumed",
        "established": "ssl_established", "server_name": "ssl_subject", "issuer": "ssl_issuer"
    },
    "http.log": {
        "ts": "ts", "id.orig_h": "src_ip", "id.orig_p": "src_port", "id.resp_h": "dst_ip", "id.resp_p": "dst_port",
        "trans_depth": "http_trans_depth", "method": "http_method", "uri": "http_uri",
        "referrer": "http_referrer", "version": "http_version", "request_body_len": "http_request_body_len",
        "response_body_len": "http_response_body_len", "status_code": "http_status_code",
        "user_agent": "http_user_agent", "orig_mime_types": "http_orig_mime_types", "resp_mime_types": "http_resp_mime_types"
    },
    "weird.log": {
        "ts": "ts", "id.orig_h": "src_ip", "id.orig_p": "src_port", 
        "id.resp_h": "dst_ip", "id.resp_p": "dst_port", 
        "name": "weird_name", "addl": "weird_addl", "notice": "weird_notice"
    }
}

# Load models and preprocessor
binary_model = joblib.load("models/binary_model_RandomForest.pkl")
multi_model = joblib.load("models/multi_model_RandomForest.pkl")
preprocessor = joblib.load("models/preprocessor.pkl")

NUMERIC_COLUMNS = [
    "ts", "src_port", "dst_port", "duration", "src_bytes", "dst_bytes",
    "missed_bytes", "src_pkts", "src_ip_bytes", "dst_pkts", "dst_ip_bytes",
    "dns_qclass", "dns_qtype", "dns_rcode", "http_request_body_len",
    "http_response_body_len", "http_status_code"
]


def get_active_interfaces():
    """
    Returns a list of active network interfaces with traffic.
    """
    interfaces = psutil.net_if_addrs()
    stats = psutil.net_if_stats()
    return [iface for iface, stat in stats.items() if stat.isup and iface in interfaces]

def get_active_interfaces():
    """
    Returns a list of active network interfaces with traffic.
    """
    interfaces = psutil.net_if_addrs()
    stats = psutil.net_if_stats()
    active_interfaces = [
        iface for iface, stat in stats.items()
        if stat.isup and iface in interfaces
    ]
    return active_interfaces


def clean_previous_files():
    """
    Removes old files in the log directory and previously generated CSV files.
    Additionally, terminates any active Zeek processes.
    """
    if os.path.exists(LOG_DIR):
        for file in os.listdir(LOG_DIR):
            file_path = os.path.join(LOG_DIR, file)
            if os.path.isfile(file_path):
                os.remove(file_path)
                print(f"File removed: {file_path}")
    else:
        os.makedirs(LOG_DIR)
        print(f"Directory created: {LOG_DIR}")

    for csv_file in [RAW_CSV, FINAL_CSV]:
        if os.path.exists(csv_file):
            os.remove(csv_file)
            print(f"File removed: {csv_file}")

    # Terminate active Zeek processes
    try:
        for proc in psutil.process_iter(['pid', 'name']):
            if 'zeek' in proc.info['name']:
                print(f"Terminating Zeek process: {proc.info['pid']}")
                proc.terminate()  # Attempt to terminate the process gracefully
                proc.wait(timeout=3)
    except Exception as e:
        print(f"Error terminating Zeek processes: {e}")

        
def start_zeek_capture(interface="en0", script_name="real-time/force_all_logs.zeek", duration=60, password=None, pcap_file=None):
    """
    Starts a Zeek capture or processes a PCAP file.
    """
    os.makedirs(LOG_DIR, exist_ok=True)  # Ensure that the LOG_DIR directory exists

    process = None  # Initialize process as None to track the main process

    if pcap_file:
        print(f"Processing PCAP file: {pcap_file}")
        try:
            subprocess.run(["zeek", "-r", pcap_file, script_name], check=True)
            print("PCAP processing completed.")
        except subprocess.CalledProcessError as e:
            print(f"Error processing PCAP file: {e}")
            exit(1)
    else:
        print(f"Starting Zeek capture on interface {interface} for {duration} seconds...")
        try:
            if password:
                command = f"echo {password} | sudo -S zeek -i {interface} {script_name}"
                process = subprocess.Popen(command, shell=True, preexec_fn=os.setsid)
            else:
                process = subprocess.Popen(["sudo", "zeek", "-i", interface, script_name], preexec_fn=os.setsid)
            
            time.sleep(duration)

            # Terminate the main process and its subprocesses
            if process:
                os.killpg(os.getpgid(process.pid), signal.SIGTERM)  # Terminate the process group
                print("Capture terminated.")
        except subprocess.CalledProcessError as e:
            print(f"Error executing Zeek: {e}")
            exit(1)

    # Add a short delay to ensure all files are generated
    time.sleep(2)

    # Move generated logs to the 'zeek_logs' directory
    for item in os.listdir("."):
        item_path = os.path.join(".", item)
        if os.path.isfile(item_path) and item.endswith(".log"):
            destination = os.path.join(LOG_DIR, item)
            try:
                os.rename(item_path, destination)
                print(f"Log moved to {LOG_DIR}: {item}")
            except Exception as e:
                print(f"Error moving log {item}: {e}")

    print("All logs have been moved to the 'zeek_logs' directory.")


def process_zeek_logs():
    """
    Processes the logs generated by Zeek in the 'real-time/zeek_logs' directory into a consolidated DataFrame.
    """
    rows = []
    for log_file in os.listdir(LOG_DIR):
        log_path = os.path.join(LOG_DIR, log_file)
        if log_file in FIELD_MAPPINGS and os.path.exists(log_path):
            print(f"Processing log: {log_file}")
            with open(log_path, "r") as file:
                headers = []
                for line in file:
                    if line.startswith("#fields"):
                        headers = line.strip().split("\t")[1:]
                    elif not line.startswith("#"):
                        fields = line.strip().split("\t")
                        # Initialize the dictionary with default values
                        row = {feature: ("0" if feature in NUMERIC_COLUMNS else "-") 
                               for feature in FIELD_MAPPINGS[log_file].values()}
                        for zeek_field, csv_field in FIELD_MAPPINGS[log_file].items():
                            if zeek_field in headers:
                                value = fields[headers.index(zeek_field)]
                                row[csv_field] = value if value.strip() else ("0" if csv_field in NUMERIC_COLUMNS else "-")
                        rows.append(row)

    if not rows:
        print("No data was extracted from Zeek logs.")
        return pd.DataFrame()

    data = pd.DataFrame(rows)

    # Ensure numeric columns are represented as strings "0"
    for col in NUMERIC_COLUMNS:
        if col in data.columns:
            data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0).astype(int).astype(str)

    # Fill missing values in categorical columns with "-"
    for col in data.columns:
        if col not in NUMERIC_COLUMNS:
            data[col] = data[col].fillna("-").astype(str)

    # Ensure the specified order for saving the CSV
    final_columns_order = [
        "ts", "src_ip", "src_port", "dst_ip", "dst_port", "proto", "service",
        "duration", "src_bytes", "dst_bytes", "conn_state", "missed_bytes",
        "src_pkts", "src_ip_bytes", "dst_pkts", "dst_ip_bytes", "dns_query",
        "dns_qclass", "dns_qtype", "dns_rcode", "dns_AA", "dns_RD", "dns_RA",
        "dns_rejected", "ssl_version", "ssl_cipher", "ssl_resumed", "ssl_established",
        "ssl_subject", "ssl_issuer", "http_trans_depth", "http_method", "http_uri",
        "http_referrer", "http_version", "http_request_body_len", "http_response_body_len",
        "http_status_code", "http_user_agent", "http_orig_mime_types", "http_resp_mime_types",
        "weird_name", "weird_addl", "weird_notice", "label", "type"
    ]

    # Ensure that all final order columns are in the DataFrame
    for col in final_columns_order:
        if col not in data.columns:
            if col in NUMERIC_COLUMNS:
                data[col] = "0"  # Default numeric values as string "0"
            else:
                data[col] = "-"  # Default categorical values as "-"

    # Reorder the columns
    data = data[final_columns_order]

    # Save the captured data before preprocessing
    raw_csv_path = RAW_CSV
    data.to_csv(raw_csv_path, index=False)
    print(f"Captured data saved to: {raw_csv_path}")

    return data

def classify_traffic(data):
    """
    Classifies real-time traffic with a ChatGPT query for anomalous traffic.
    """
    if data.empty:
        print("No data to classify.")
        return []
    
    predicted_data = pd.DataFrame()  # Safe initialization

    # Add missing columns with default values
    expected_columns = ["ts", "src_ip", "src_port", "dst_ip", "dst_port", "proto", "service",
                        "duration", "src_bytes", "dst_bytes", "conn_state", "missed_bytes",
                        "src_pkts", "src_ip_bytes", "dst_pkts", "dst_ip_bytes", "dns_query",
                        "dns_qclass", "dns_qtype", "dns_rcode", "dns_AA", "dns_RD", "dns_RA",
                        "dns_rejected", "ssl_version", "ssl_cipher", "ssl_resumed", "ssl_established",
                        "ssl_subject", "ssl_issuer", "http_trans_depth", "http_method", "http_uri",
                        "http_referrer", "http_version", "http_user_agent", "http_orig_mime_types",
                        "http_resp_mime_types", "http_request_body_len", "http_response_body_len",
                        "http_status_code"]

    for col in expected_columns:
        if col not in data.columns:
            if (col.startswith("http") and col not in ["http_trans_depth", "http_user_agent"]) or col in ["duration", "src_bytes", "dst_bytes"]:
                data[col] = 0
            else:
                data[col] = "-"


    # Identify categorical and numeric columns
    categorical_cols = data.select_dtypes(include=["object"]).columns.tolist()
    numerical_cols = data.select_dtypes(include=["number"]).columns.tolist()

    # Correct problematic values in specific columns
    for col in ["src_port", "dst_port"]:
        data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0).astype(int)

    for col in ["dst_bytes", "missed_bytes", "src_pkts", "src_ip_bytes"]:
        data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0).astype(float)

    for col in ["dns_rcode"]:
        data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0).astype(float)

    # Ensure that `http_user_agent` and `http_trans_depth` are filled correctly
    for col in ["http_user_agent", "http_trans_depth"]:
        if col in data.columns:
            data[col] = data[col].apply(lambda x: "-" if pd.isna(x) else x).astype(str)
        else:
            data[col] = "-"

    # Convert categorical columns to numeric values
    for col in categorical_cols:
        if col in preprocessor["label_encoders"]:
            unseen_value = "-"
            data[col] = data[col].apply(lambda x: x if x in preprocessor["label_encoders"][col].classes_ else unseen_value)
            if unseen_value not in preprocessor["label_encoders"][col].classes_:
                preprocessor["label_encoders"][col].classes_ = np.append(preprocessor["label_encoders"][col].classes_, unseen_value)
            data[col] = preprocessor["label_encoders"][col].transform(data[col])
        else:
            data[col] = data[col].fillna("-").astype(str)

    # Convert numeric columns to float and replace invalid values
    for col in numerical_cols:
        data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0)

    # Diagnostics: check columns and types before applying the scaler
    print("Data before applying the scaler:")
    print(data.info())
    print(data.head())

    # Check the features expected by the scaler
    scaler = preprocessor["scaler"]
    print("Feature names expected by the scaler:")
    print(scaler.feature_names_in_)

    # Debug: Compare features expected by the scaler with the current DataFrame columns
    expected_features = set(scaler.feature_names_in_)
    actual_features = set(data.columns)
    missing_features = expected_features - actual_features
    extra_features = actual_features - expected_features
    print("Expected features by the scaler:", scaler.feature_names_in_)
    print("Current DataFrame features:", list(data.columns))
    print("Missing features:", missing_features)
    print("Extra features:", extra_features)

    # Ensure that the columns match those of the scaler
    for col in scaler.feature_names_in_:
        if col not in data.columns:
            print(f"Column '{col}' is missing in the DataFrame. Adding it with default value 0.")
            data[col] = 0
        elif not pd.api.types.is_numeric_dtype(data[col]):
            print(f"Column '{col}' contains non-numeric values:")
            print(data[col].unique())

    # Reorder the columns to match the scaler
    data = data[scaler.feature_names_in_]

    preprocessed_csv_path = "real-time/preprocessed_capture_data.csv"
    predicted_data.to_csv(preprocessed_csv_path, index=False)
    print(f"Final data with predictions saved to: {preprocessed_csv_path}")

    # Apply the scaler
    X_preprocessed = scaler.transform(data)

    # Classify traffic
    label_pred = binary_model.predict(X_preprocessed)
    results = []
    predicted_data = data.copy()

    for idx, probabilities in enumerate(binary_model.predict_proba(X_preprocessed)):
        benign_confidence = probabilities[0]  # Confidence for "benign"
        malign_confidence = probabilities[1]  # Confidence for "malignant"

        if benign_confidence > malign_confidence:  # Benign with high confidence
            results.append({
                "label": "benigno", 
                "type": "normal", 
                "confidence": benign_confidence
            })
            predicted_data.at[idx, "label"] = 0
            predicted_data.at[idx, "type"] = "normal"
            predicted_data.at[idx, "confidence"] = benign_confidence
            
        elif malign_confidence < 0.8:  # Benign with low confidence
            results.append({
                "label": "benigno", 
                "type": "normal", 
                "confidence": benign_confidence
            })
            predicted_data.at[idx, "label"] = 0
            predicted_data.at[idx, "type"] = "normal"
            predicted_data.at[idx, "confidence"] = benign_confidence

        else:  # Malignant with high confidence
            type_pred_proba = multi_model.predict_proba([X_preprocessed[idx]])[0]
            type_pred = multi_model.classes_[type_pred_proba.argmax()]
            attack_confidence = type_pred_proba.max()

            if malign_confidence < 0.85:  # Confidence threshold for malignant
                results.append({
                    "label": "maligno", 
                    "type": "Unknown Traffic", 
                    "confidence": malign_confidence
                })
                predicted_data.at[idx, "label"] = 1
                predicted_data.at[idx, "type"] = "Unknown Traffic"
                predicted_data.at[idx, "confidence"] = malign_confidence
            else:
                results.append({
                    "label": "maligno", 
                    "type": type_pred, 
                    "confidence": attack_confidence
                })
                predicted_data.at[idx, "label"] = 1
                predicted_data.at[idx, "type"] = type_pred
                predicted_data.at[idx, "confidence"] = attack_confidence

    # Save the results
    final_csv_path = "real-time/final_captured_data.csv"
    predicted_data.to_csv(final_csv_path, index=False)
    print(f"Final data with predictions saved to: {final_csv_path}")

    # Return results and data for graphing
    graph_data = generate_graph_data(predicted_data)
    return results, graph_data


def generate_graph_data(data):
    """
    Processes the data and returns the distribution of benign and malignant packets
    and the types of attacks for use in the interface.
    """
    benign_count = data[data["label"] == 0].shape[0]
    malign_count = data[data["label"] == 1].shape[0]

    attack_types = (
        data[data["label"] == 1]["type"].value_counts().to_dict()
        if malign_count > 0
        else {}
    )

    return {
        "benign_count": benign_count,
        "malign_count": malign_count,
        "attack_types": attack_types,
    }


def main():
    clean_previous_files()

    interface = input("Enter the network interface (e.g., en0): ")
    duration = int(input("Enter the capture duration (seconds): "))
    start_zeek_capture(interface=interface, duration=duration)
    data = process_zeek_logs()
    results = classify_traffic(data)

if __name__ == "__main__":
    main()
